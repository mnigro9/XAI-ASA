{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmQIAFTT39cy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load drive, functions, libraries"
      ],
      "metadata": {
        "id": "87F9ln7W4Lfj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6S2qsrEnAVh"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "reIDRLML5bJX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDMHXV0RnWZe"
      },
      "outputs": [],
      "source": [
        "!pip install -q keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from tensorflow.python.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import BatchNormalization, Bidirectional, TimeDistributed\n",
        "#from tensorflow.python.keras.layers import Dense, Flatten, Permute, Reshape, Activation, add, MaxPooling1D, concatenate, GlobalAveragePooling2D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Dense, Flatten, Permute, Reshape, Activation, add, GlobalMaxPooling1D, concatenate, GlobalAveragePooling2D, GlobalAveragePooling1D, Dropout, Conv1D, Conv2D, MaxPooling2D, Input, Multiply\n",
        "#from tensorflow.python.keras.layers import Dropout, Conv1D, Conv2D, MaxPooling2D, MaxPooling3D, AveragePooling2D, Input, Multiply\n",
        "#from tensorflow.python.keras.callbacks import *\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers.experimental import SyncBatchNormalization #Without tf.distribute strategy, this layer behaves as a regular tf.keras.layers.BatchNormalization layer....used to save model\n"
      ],
      "metadata": {
        "id": "_h_VMOeyK83S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYOC-7sQnZk6"
      },
      "outputs": [],
      "source": [
        "#import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, roc_curve, auc, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score, average_precision_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "#import seaborn as sns\n",
        "\n",
        "#sns.set(font_scale = 1.7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLuhf-QB9qIl"
      },
      "outputs": [],
      "source": [
        "def pa1(preds, test_sc_labelcat,numClasses): #source counting accuracy metric function. Input predicted labels, true labels, and number of classes\n",
        "  if len(preds.shape)==1:\n",
        "    yhat = preds\n",
        "  else:\n",
        "    yhat = np.argmax(preds,axis=1) #predicted labels\n",
        "  if len(test_sc_labelcat.shape)==1:\n",
        "    y = test_sc_labelcat\n",
        "  else:\n",
        "    y = np.argmax(test_sc_labelcat,axis=1)  #true labels\n",
        "\n",
        "  score=0 #for  pa1\n",
        "  paw_score = 0\n",
        "  values, counts = np.unique(y, return_counts=True) #'counts' to get the number of samples per class\n",
        "  perclass_acc_plusminus1 = np.zeros(numClasses) #variable to store per class accuracies plus/minus 1\n",
        "  perclass_acc = np.zeros(len(values))\n",
        "\n",
        "  cm = confusion_matrix(y,yhat)\n",
        "  print(f'Average Accuracy: {np.trace(cm)/len(y)*100}%')\n",
        "  for i in range(len(cm)):\n",
        "    print(f'Accuracy per class source count {i+1}: {cm[i,i]/np.sum(cm[i,:])*100}%')\n",
        "\n",
        "  print('\\n Plus/minus 1 stats:')\n",
        "  for i in range(len(y)):\n",
        "    plus1 = yhat[i]+1\n",
        "    minus1 = yhat[i]-1\n",
        "    if yhat[i]==y[i]:\n",
        "      perclass_acc[y[i]]+=1\n",
        "      score=score+1\n",
        "      paw_score+=1\n",
        "      perclass_acc_plusminus1[y[i]]+=1\n",
        "    if plus1==y[i]:\n",
        "      score=score+1\n",
        "      paw_score = paw_score+0.5\n",
        "      perclass_acc_plusminus1[y[i]]+=1\n",
        "    if minus1==y[i]:\n",
        "      score=score+1\n",
        "      paw_score = paw_score+0.5\n",
        "      perclass_acc_plusminus1[y[i]]+=1\n",
        "  accuracy_plusminus1 = score/len(y) *100\n",
        "  print(f'Accuracy plus/minus 1 source counted is: {accuracy_plusminus1}')\n",
        "  print('Per class accuracy plus/minus 1 source counted.')\n",
        "  for i in range(len(perclass_acc_plusminus1)):\n",
        "    print(f' Class Source count {i+1}: {perclass_acc_plusminus1[i]/counts[i]*100}')\n",
        "  print('\\n----- PAW1 -----')\n",
        "  print(f'Average PAW1: {paw_score/len(y) *100}')\n",
        "  paw = (perclass_acc + perclass_acc_plusminus1)/2\n",
        "  for j in range(len(paw)):\n",
        "    print(f'PAW1 Class Source count {j+1}: {paw[j]/counts[j]*100}')\n",
        "  return accuracy_plusminus1, perclass_acc_plusminus1/counts*100\n",
        "\n",
        "\n",
        "# NN visualization\n",
        "'''\n",
        "from keras.utils import plot_model\n",
        "plot_model(model, 'model.png')\n",
        "'''\n",
        "def model_plot(history):\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def plot_result(item):\n",
        "  '''\n",
        "  input: 'item' as the name for the metric to plot i.e. 'loss' or 'accuracy'\n",
        "  '''\n",
        "  plt.plot(history.history[item], label=item)\n",
        "  plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(item)\n",
        "  plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "## Test set accuracy and confusion\n",
        "def results(preds,test_labelcat,class_names):\n",
        "  #input the predictions as one hot output predictions, true labels as one hot categorical\n",
        "  if len(preds.shape)==1:\n",
        "    yhat = preds\n",
        "  else:\n",
        "    yhat = np.argmax(preds,axis=1) #predicted labels\n",
        "  if len(test_labelcat.shape)==1:\n",
        "    y = test_labelcat\n",
        "  else:\n",
        "    y = np.argmax(test_labelcat,axis=1)  #true labels\n",
        "\n",
        "  #y = np.argmax(test_labelcat,axis=1) #true labels\n",
        "  #yhat = np.argmax(preds,axis=1) #predicted labels\n",
        "  print(accuracy_score(y,yhat))\n",
        "  #print(confusion_matrix(y,yhat))\n",
        "  print(classification_report(y, yhat, target_names=class_names))\n",
        "  cm = confusion_matrix(y,yhat)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "  disp.plot()\n",
        "  plt.show()\n",
        "\n",
        "  print('\\n------ MCC -----')\n",
        "  print(matthews_corrcoef(y, yhat))\n",
        "\n",
        "  print('\\n------ F1-score ------')\n",
        "  print(f1_score(y,yhat, average='micro')) #micro weighing; can use 'macro' or 'weighted'\n",
        "\n",
        "  print('\\n------ AUC -------')\n",
        "  print(roc_auc_score(test_labelcat,preds,average='micro'))\n",
        "\n",
        "  from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "  RocCurveDisplay.from_predictions(test_labelcat.ravel(), preds.ravel())\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def mse_mae_(test_sc_labelcat,preds,cmin):\n",
        "  # categorical true labels, model predictions, 'cmin': min value fo the classes\n",
        "  from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "  y = np.argmax(test_sc_labelcat,axis=1)  #true labels\n",
        "  #preds = modelB.predict(test)\n",
        "  yhat = np.argmax(preds,axis=1) #predicted labels\n",
        "  numClasses = len(np.unique(y))\n",
        "  print('Overall error metrics')\n",
        "  print('MSE', mean_squared_error(y,yhat))\n",
        "  print('MAE', mean_absolute_error(y,yhat))  #MAE reported in visual counting paper along with the OBO metric\n",
        "  print('\\n')\n",
        "  for i in range(numClasses):\n",
        "    y_true = y[np.where(y==i)]  #getting labels for specific class\n",
        "    y_pred = yhat[np.where(y==i)] #getting corresponding predictions for specific class\n",
        "    print('Class', i+cmin)\n",
        "    print('MSE', mean_squared_error(y_true,y_pred))\n",
        "    print('MAE', mean_absolute_error(y_true,y_pred))\n",
        "\n",
        "def conf_nice(fine_preds, test_sc_labelcat, numClasses): #better version of above function\n",
        "  y = np.argmax(test_sc_labelcat,axis=1)  #true labels\n",
        "  yhat = np.argmax(fine_preds,axis=1) #predicted labels\n",
        "  score=0\n",
        "  #numClasses=12\n",
        "  values, counts = np.unique(y, return_counts=True) #'counts' to get the number of samples per class\n",
        "  perclass_acc_plusminus1 = np.zeros(numClasses) #variable to store per class accuracies plus/minus 1\n",
        "  labels_range = [*range(1,numClasses+1)] #getting a list of the number of possible source counts\n",
        "  disp = ConfusionMatrixDisplay.from_predictions(y+1, yhat+1, labels = labels_range,\n",
        "                                                 normalize='true',\n",
        "                                                 values_format='.1') #give the true labels and predicted labels. '+1' so its the correct range of source count problem where counting doesn't start at 0\n",
        "  fig = disp.figure_\n",
        "  fig.set_figwidth(10)\n",
        "  fig.set_figheight(10)\n",
        "  #plt.figure(figsize=(10,10))\n",
        "  #disp.plot()\n",
        "  plt.show()\n",
        "\n",
        "def get_metrics(y_test,preds, class_names):\n",
        "  # function to get sklearn metrics: classification report, confusion matrix, fscore, MCC, ROC-AUC\n",
        "  # input: labels as numeric, and model predictions as numeric\n",
        "  # class_names: list of the labels in sequence order\n",
        "\n",
        "  from sklearn.metrics import matthews_corrcoef, f1_score, roc_curve, auc, roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "  from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "  label_binarizer = LabelBinarizer().fit(y_test)  #needed for roc, auc. labels in shape of (num of samples, num of classes)\n",
        "  y_onehot = label_binarizer.transform(y_test)\n",
        "  preds_onehot = label_binarizer.transform(preds)\n",
        "\n",
        "  print(classification_report(y_test, preds, target_names=class_names))\n",
        "  cm = confusion_matrix(y_test,preds)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "  disp.plot()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  print('\\n------ MCC -----')\n",
        "  print(matthews_corrcoef(y_test, preds))\n",
        "\n",
        "  print('\\n------ F1-score ------')\n",
        "  print(f1_score(y_test,preds)) #micro weighing; can use 'macro' or 'weighted'\n",
        "\n",
        "  print('\\n------ AUC -------')\n",
        "  print(roc_auc_score(y_onehot,preds_onehot))\n",
        "\n",
        "def calculate_overall_lwlrap_sklearn(truth, scores):\n",
        "  \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n",
        "  import sklearn.metrics\n",
        "  # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n",
        "  sample_weight = np.sum(truth > 0, axis=1)\n",
        "  nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
        "  overall_lwlrap = sklearn.metrics.label_ranking_average_precision_score(\n",
        "      truth[nonzero_weight_sample_indices, :] > 0,\n",
        "      scores[nonzero_weight_sample_indices, :],\n",
        "      sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
        "  return overall_lwlrap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to make HCF"
      ],
      "metadata": {
        "id": "XslAFQAw4qbt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNCIeroR2ATP"
      },
      "source": [
        "from scipy import signal\n",
        "from scipy.stats import kurtosis\n",
        "import librosa\n",
        "#https://librosa.org/doc/latest/feature.html\n",
        "#Function: from a list of audio files extract acoustic features using Librosa\n",
        "\n",
        "def acoustics(train_audio):\n",
        "  #root mean square: takes raw audio signal, makes overlapping frames, and computes sqrt(power) of each frame as RMS\n",
        "  rms_list = np.array([librosa.feature.rms(y=audio, frame_length=2048, hop_length=1024)\n",
        "                     for audio in train_audio])\n",
        "#spectral centroid: given audio signal, mag spectrogram computed.\n",
        "# Each frame of a magnitude spectrogram is normalized and treated as a distribution over frequency bins,\n",
        "# from which the mean (centroid) isextracted per frame and returned as a vector.\n",
        "  cs_list = np.array([librosa.feature.spectral_centroid(y=audio, n_fft=2048, hop_length=1024)\n",
        "                    for audio in train_audio])\n",
        "#spectral bandwidth\n",
        "  bw_list = np.array([librosa.feature.spectral_bandwidth(y=audio, n_fft=2048, hop_length=1024)\n",
        "                    for audio in train_audio])\n",
        "#spectral flatness\n",
        "  fl_list = np.array([librosa.feature.spectral_flatness(y=audio, n_fft=2048, hop_length=1024)\n",
        "                    for audio in train_audio])\n",
        "#spectral rolloff\n",
        "  roll_list = np.array([librosa.feature.spectral_rolloff(y=audio, n_fft=2048, hop_length=1024)\n",
        "                      for audio in train_audio])\n",
        "\n",
        "\n",
        "#to fix the shape?\n",
        "  rms_list = rms_list[:,0,:]\n",
        "  cs_list = cs_list[:,0,:]\n",
        "  bw_list = bw_list[:,0,:]\n",
        "  fl_list = fl_list[:,0,:]\n",
        "  roll_list = roll_list[:,0,:]\n",
        "\n",
        "  return rms_list,cs_list,bw_list,fl_list,roll_list\n",
        "\n",
        "\n",
        "#additional features from the STFT, called in 'spec_features'\n",
        "def get0_power(a, threshold):\n",
        "    mint = -80 ## minimum threshold\n",
        "    if threshold == mint:\n",
        "        idx = np.where(a == threshold)[0]\n",
        "    else:\n",
        "        idx = np.where(a <= threshold)[0]\n",
        "    if len(idx) > 1:\n",
        "        idx = idx[0]\n",
        "    elif len(idx) == 0:\n",
        "        idx = 128 #len(a)\n",
        "    return(idx)\n",
        "\n",
        "\n",
        "#mel spectrogram calculation features\n",
        "def spec_features(audio_list): #input list of all audio signals\n",
        "\n",
        "  stft_list = np.array([librosa.feature.melspectrogram(y=audio, sr=44100,\n",
        "                                                     n_mels=64, fmax=22050, #fmax=sr/2\n",
        "                                                     n_fft=2048,\n",
        "                                                     hop_length=1024)\n",
        "                          for audio in audio_list])\n",
        "  print('stft done')\n",
        "  #convert melspec to dB scale\n",
        "  stft_list_dB = np.array([librosa.power_to_db(stft_list[i,:,:], ref=np.max)\n",
        "                          for i in range(stft_list.shape[0])])\n",
        "  print('stft db done')\n",
        "\n",
        "  ## median power vs time\n",
        "  medp_time = np.array([np.apply_along_axis(np.median,  0, stft_list_dB[i,:,:])\n",
        "                      for i in range(stft_list.shape[0])])\n",
        "  ## mean power vs. time\n",
        "  meanp_time = np.array([np.apply_along_axis(np.mean,  0, stft_list_dB[i,:,:])\n",
        "                       for i in range(stft_list.shape[0])])\n",
        "  print('power vs time done')\n",
        "\n",
        "  ## time to power of -80\n",
        "  tp80 = np.array([np.apply_along_axis(get0_power,  1, stft_list_dB[i,:,:], -80).flatten()\n",
        "                 for i in range(stft_list.shape[0])])\n",
        "  ## time to power of -75\n",
        "  tp75 = np.array([np.apply_along_axis(get0_power,  1, stft_list_dB[i,:,:], -75).flatten()\n",
        "                 for i in range(stft_list.shape[0])])\n",
        "  ## time to power of -70\n",
        "  tp70 = np.array([np.apply_along_axis(get0_power,  1, stft_list_dB[i,:,:], -70).flatten()\n",
        "                 for i in range(stft_list.shape[0])])\n",
        "  print('time to 80,75, 70 done')\n",
        "\n",
        "\n",
        "  #### wavelet features #### looking at widths of 1, 5, 10, 25\n",
        "  width = 1\n",
        "  addwavelets_1 = np.zeros((7, stft_list_dB.shape[0], stft_list_dB.shape[1]))\n",
        "\n",
        "  for j in range(stft_list_dB.shape[0]):\n",
        "    wavetransform = np.array([signal.cwt(stft_list_dB[j, i, :], signal.ricker,[width])[0]\n",
        "                              for i in range(stft_list_dB.shape[1])])\n",
        "    addwavelets_1[0, j,:] = np.mean(wavetransform, axis = 1) #mean wavelet\n",
        "    addwavelets_1[1, j,:] = np.median(wavetransform, axis = 1) #median wavelet\n",
        "    addwavelets_1[2, j,:] = np.std(wavetransform, axis = 1) #standard dev\n",
        "    addwavelets_1[3, j,:] = np.var(wavetransform, axis = 1) #variance\n",
        "    addwavelets_1[4, j,:] = kurtosis(wavetransform, axis = 1) #kurtosis\n",
        "    addwavelets_1[5, j,:] = np.quantile(wavetransform, 0.25, axis = 1) #25th quantile\n",
        "    addwavelets_1[6, j,:] = np.quantile(wavetransform, 0.75, axis = 1) #75th quantile...need to explain significance of these\n",
        "    if j % 1000 == 0:\n",
        "        print(j)\n",
        "  print('wavelet 1 done')\n",
        "\n",
        "  width = 5\n",
        "  addwavelets_5 = np.zeros((7, stft_list_dB.shape[0], stft_list_dB.shape[1]))\n",
        "\n",
        "  for j in range(stft_list_dB.shape[0]):\n",
        "    wavetransform = np.array([signal.cwt(stft_list_dB[j, i, :], signal.ricker,[width])[0]\n",
        "                              for i in range(stft_list_dB.shape[1])])\n",
        "    addwavelets_5[0, j,:] = np.mean(wavetransform, axis = 1) #mean wavelet\n",
        "    addwavelets_5[1, j,:] = np.median(wavetransform, axis = 1) #median wavelet\n",
        "    addwavelets_5[2, j,:] = np.std(wavetransform, axis = 1) #standard dev\n",
        "    addwavelets_5[3, j,:] = np.var(wavetransform, axis = 1) #variance\n",
        "    addwavelets_5[4, j,:] = kurtosis(wavetransform, axis = 1) #kurtosis\n",
        "    addwavelets_5[5, j,:] = np.quantile(wavetransform, 0.25, axis = 1) #25th quantile\n",
        "    addwavelets_5[6, j,:] = np.quantile(wavetransform, 0.75, axis = 1) #75th quantile...need to explain significance of these\n",
        "    if j % 1000 == 0:\n",
        "        print(j)\n",
        "  print('wavelet 5 done')\n",
        "\n",
        "\n",
        "  width = 10\n",
        "  addwavelets_10 = np.zeros((7, stft_list_dB.shape[0], stft_list_dB.shape[1]))\n",
        "\n",
        "  for j in range(stft_list_dB.shape[0]):\n",
        "    wavetransform = np.array([signal.cwt(stft_list_dB[j, i, :], signal.ricker,[width])[0]\n",
        "                              for i in range(stft_list_dB.shape[1])])\n",
        "    addwavelets_10[0, j,:] = np.mean(wavetransform, axis = 1) #mean wavelet\n",
        "    addwavelets_10[1, j,:] = np.median(wavetransform, axis = 1) #median wavelet\n",
        "    addwavelets_10[2, j,:] = np.std(wavetransform, axis = 1) #standard dev\n",
        "    addwavelets_10[3, j,:] = np.var(wavetransform, axis = 1) #variance\n",
        "    addwavelets_10[4, j,:] = kurtosis(wavetransform, axis = 1) #kurtosis\n",
        "    addwavelets_10[5, j,:] = np.quantile(wavetransform, 0.25, axis = 1) #25th quantile\n",
        "    addwavelets_10[6, j,:] = np.quantile(wavetransform, 0.75, axis = 1) #75th quantile...need to explain significance of these\n",
        "    if j % 1000 == 0:\n",
        "        print(j)\n",
        "  print('wavelet 10 done')\n",
        "\n",
        "  width = 25\n",
        "  addwavelets_25 = np.zeros((7, stft_list_dB.shape[0], stft_list_dB.shape[1]))\n",
        "\n",
        "  for j in range(stft_list_dB.shape[0]):\n",
        "    wavetransform = np.array([signal.cwt(stft_list_dB[j, i, :], signal.ricker,[width])[0]\n",
        "                              for i in range(stft_list_dB.shape[1])])\n",
        "    addwavelets_25[0, j,:] = np.mean(wavetransform, axis = 1) #mean wavelet\n",
        "    addwavelets_25[1, j,:] = np.median(wavetransform, axis = 1) #median wavelet\n",
        "    addwavelets_25[2, j,:] = np.std(wavetransform, axis = 1) #standard dev\n",
        "    addwavelets_25[3, j,:] = np.var(wavetransform, axis = 1) #variance\n",
        "    addwavelets_25[4, j,:] = kurtosis(wavetransform, axis = 1) #kurtosis\n",
        "    addwavelets_25[5, j,:] = np.quantile(wavetransform, 0.25, axis = 1) #25th quantile\n",
        "    addwavelets_25[6, j,:] = np.quantile(wavetransform, 0.75, axis = 1) #75th quantile...need to explain significance of these\n",
        "    if j % 1000 == 0:\n",
        "        print(j)\n",
        "\n",
        "  return stft_list, stft_list_dB, medp_time, meanp_time, tp80, tp75, tp70, addwavelets_1, addwavelets_5, addwavelets_10, addwavelets_25\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# additional acoustic features;\n",
        "# treat mfccs just like the wavelet based features (can take stats from it like mean, std, kurt,)\n",
        "import numpy as np\n",
        "import librosa\n",
        "import pandas as pd\n",
        "###################################\n",
        "import sys\n",
        "eps = sys.float_info.epsilon\n",
        "\n",
        "def additional_features(audio, split, split_scene,frame_len, hop_len):\n",
        "  sr = 44100\n",
        "  frame_len = 2048\n",
        "  hop_len = 1024\n",
        "  flag = 0\n",
        "  mfcc=[]\n",
        "  mfcc_d=[]\n",
        "  mfcc_dd=[]\n",
        "\n",
        "  #for count,y in enumerate(audio): #list of raw audio signals\n",
        "  for i in range(len(audio)): #dataframe to get list of audio file names\n",
        "    audiofile = df['Filename'][i].split(\"/\") #get files in order, split string for audio file only not path\n",
        "    f = audiofile[7]\n",
        "    filename = f'/content/drive/My Drive/SARdBScene/{split}/{split_scene}/{f}'\n",
        "    y,sr = librosa.load(filename,sr=44100)\n",
        "\n",
        "\n",
        "    S = np.abs(librosa.stft(y,n_fft=frame_len, hop_length=hop_len)) #gets the STFT mag or spectrogram (rows are freq. axis=0)\n",
        "    print(i)\n",
        "\n",
        "    m = librosa.feature.mfcc(y=y, sr=sr,n_fft=frame_len, hop_length=hop_len, n_mfcc=13)\n",
        "    mfcc.append(m)\n",
        "    mfcc_d.append(librosa.feature.delta(m))\n",
        "    mfcc_dd.append(librosa.feature.delta(librosa.feature.delta(m)))\n",
        "\n",
        "    if flag==0: #only for the first data sample, then we concat the features for one storage variable\n",
        "      # zero crossing rate\n",
        "      zcr = librosa.feature.zero_crossing_rate(y,frame_length = frame_len, hop_length=hop_len)\n",
        "\n",
        "      #\n",
        "      # 1) compute probabilty distribution of spectrogram\n",
        "      p = S / (np.sum(S,axis=0)+eps)\n",
        "      # 2) calculate entropy of each time frame\n",
        "      H = -np.sum(p * np.log2(p+eps),axis=0).reshape((1,431))\n",
        "\n",
        "    elif flag>0:\n",
        "      zcr = np.concatenate((zcr,librosa.feature.zero_crossing_rate(y,frame_length = frame_len, hop_length=hop_len)),axis=0)\n",
        "      m = librosa.feature.mfcc(y=y, sr=sr,n_fft=frame_len, hop_length=hop_len, n_mfcc=13)\n",
        "      #\n",
        "      p = S / (np.sum(S,axis=0)+eps)\n",
        "      H = np.concatenate((H, -np.sum(p*np.log2(p+eps),axis=0).reshape((1,431))),axis=0)\n",
        "\n",
        "    flag = 10\n",
        "\n",
        "  mfcc = np.reshape(np.array([mfcc]),(len(audio),13,431))  #make as array type and reshape to remove additional axis\n",
        "  mfcc_d = np.reshape(np.array([mfcc_d]),(len(audio),13,431))\n",
        "  mfcc_dd = np.reshape(np.array([mfcc_dd]),(len(audio),13,431))\n",
        "\n",
        "  return zcr, mfcc, mfcc_d, mfcc_dd, H\n",
        "\n"
      ],
      "metadata": {
        "id": "RICUm-ZAKqOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CKA Function"
      ],
      "metadata": {
        "id": "czW3xC0_5WwZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkucRi3yn7UJ"
      },
      "outputs": [],
      "source": [
        "#https://colab.research.google.com/github/google-research/google-research/blob/master/representation_similarity/Demo.ipynb#scrollTo=MkucRi3yn7UJ\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "def gram_linear(x):\n",
        "  \"\"\"Compute Gram (kernel) matrix for a linear kernel.\n",
        "\n",
        "  Args:\n",
        "    x: A num_examples x num_features matrix of features.\n",
        "\n",
        "  Returns:\n",
        "    A num_examples x num_examples Gram matrix of examples.\n",
        "  \"\"\"\n",
        "  return x.dot(x.T)\n",
        "\n",
        "\n",
        "def gram_rbf(x, threshold=1.0):\n",
        "  \"\"\"Compute Gram (kernel) matrix for an RBF kernel.\n",
        "\n",
        "  Args:\n",
        "    x: A num_examples x num_features matrix of features.\n",
        "    threshold: Fraction of median Euclidean distance to use as RBF kernel\n",
        "      bandwidth. (This is the heuristic we use in the paper. There are other\n",
        "      possible ways to set the bandwidth; we didn't try them.)\n",
        "\n",
        "  Returns:\n",
        "    A num_examples x num_examples Gram matrix of examples.\n",
        "  \"\"\"\n",
        "  dot_products = x.dot(x.T)\n",
        "  sq_norms = np.diag(dot_products)\n",
        "  sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]\n",
        "  sq_median_distance = np.median(sq_distances)\n",
        "  return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))\n",
        "\n",
        "\n",
        "def center_gram(gram, unbiased=False):\n",
        "  \"\"\"Center a symmetric Gram matrix.\n",
        "\n",
        "  This is equvialent to centering the (possibly infinite-dimensional) features\n",
        "  induced by the kernel before computing the Gram matrix.\n",
        "\n",
        "  Args:\n",
        "    gram: A num_examples x num_examples symmetric matrix.\n",
        "    unbiased: Whether to adjust the Gram matrix in order to compute an unbiased\n",
        "      estimate of HSIC. Note that this estimator may be negative.\n",
        "\n",
        "  Returns:\n",
        "    A symmetric matrix with centered columns and rows.\n",
        "  \"\"\"\n",
        "  if not np.allclose(gram, gram.T):\n",
        "    raise ValueError('Input must be a symmetric matrix.')\n",
        "  gram = gram.copy()\n",
        "\n",
        "  if unbiased:\n",
        "    # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.\n",
        "    # L. (2014). Partial distance correlation with methods for dissimilarities.\n",
        "    # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically\n",
        "    # stable than the alternative from Song et al. (2007).\n",
        "    n = gram.shape[0]\n",
        "    np.fill_diagonal(gram, 0)\n",
        "    means = np.sum(gram, 0, dtype=np.float64) / (n - 2)\n",
        "    means -= np.sum(means) / (2 * (n - 1))\n",
        "    gram -= means[:, None]\n",
        "    gram -= means[None, :]\n",
        "    np.fill_diagonal(gram, 0)\n",
        "  else:\n",
        "    means = np.mean(gram, 0, dtype=np.float64)\n",
        "    means -= np.mean(means) / 2\n",
        "    gram -= means[:, None]\n",
        "    gram -= means[None, :]\n",
        "\n",
        "  return gram\n",
        "\n",
        "\n",
        "def cka(gram_x, gram_y, debiased=False):\n",
        "  \"\"\"Compute CKA.\n",
        "\n",
        "  Args:\n",
        "    gram_x: A num_examples x num_examples Gram matrix.\n",
        "    gram_y: A num_examples x num_examples Gram matrix.\n",
        "    debiased: Use unbiased estimator of HSIC. CKA may still be biased.\n",
        "\n",
        "  Returns:\n",
        "    The value of CKA between X and Y.\n",
        "  \"\"\"\n",
        "  gram_x = center_gram(gram_x, unbiased=debiased)\n",
        "  gram_y = center_gram(gram_y, unbiased=debiased)\n",
        "\n",
        "  # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or\n",
        "  # n*(n-3) (unbiased variant), but this cancels for CKA.\n",
        "  scaled_hsic = gram_x.ravel().dot(gram_y.ravel())\n",
        "\n",
        "  normalization_x = np.linalg.norm(gram_x)\n",
        "  normalization_y = np.linalg.norm(gram_y)\n",
        "  return scaled_hsic / (normalization_x * normalization_y)\n",
        "\n",
        "\n",
        "def _debiased_dot_product_similarity_helper(\n",
        "    xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y,\n",
        "    n):\n",
        "  \"\"\"Helper for computing debiased dot product similarity (i.e. linear HSIC).\"\"\"\n",
        "  # This formula can be derived by manipulating the unbiased estimator from\n",
        "  # Song et al. (2007).\n",
        "  return (\n",
        "      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)\n",
        "      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))\n",
        "\n",
        "\n",
        "def feature_space_linear_cka(features_x, features_y, debiased=False):\n",
        "  \"\"\"Compute CKA with a linear kernel, in feature space.\n",
        "\n",
        "  This is typically faster than computing the Gram matrix when there are fewer\n",
        "  features than examples.\n",
        "\n",
        "  Args:\n",
        "    features_x: A num_examples x num_features matrix of features.\n",
        "    features_y: A num_examples x num_features matrix of features.\n",
        "    debiased: Use unbiased estimator of dot product similarity. CKA may still be\n",
        "      biased. Note that this estimator may be negative.\n",
        "\n",
        "  Returns:\n",
        "    The value of CKA between X and Y.\n",
        "  \"\"\"\n",
        "  features_x = features_x - np.mean(features_x, 0, keepdims=True)\n",
        "  features_y = features_y - np.mean(features_y, 0, keepdims=True)\n",
        "\n",
        "  dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2\n",
        "  normalization_x = np.linalg.norm(features_x.T.dot(features_x))\n",
        "  normalization_y = np.linalg.norm(features_y.T.dot(features_y))\n",
        "\n",
        "  if debiased:\n",
        "    n = features_x.shape[0]\n",
        "    # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.\n",
        "    sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)\n",
        "    sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)\n",
        "    squared_norm_x = np.sum(sum_squared_rows_x)\n",
        "    squared_norm_y = np.sum(sum_squared_rows_y)\n",
        "\n",
        "    dot_product_similarity = _debiased_dot_product_similarity_helper(\n",
        "        dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,\n",
        "        squared_norm_x, squared_norm_y, n)\n",
        "    normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(\n",
        "        normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,\n",
        "        squared_norm_x, squared_norm_x, n))\n",
        "    normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(\n",
        "        normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,\n",
        "        squared_norm_y, squared_norm_y, n))\n",
        "\n",
        "  return dot_product_similarity / (normalization_x * normalization_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obJCdw02-dcn"
      },
      "source": [
        "Linear CKA can be computed either based on dot products between examples or dot products between features:\n",
        "$$\\langle\\text{vec}(XX^\\text{T}),\\text{vec}(YY^\\text{T})\\rangle = ||Y^\\text{T}X||_\\text{F}^2$$\n",
        "The formulation based on similarities between features (right-hand side) is faster than the formulation based on similarities between similarities between examples (left-hand side) when the number of examples exceeds the number of features. We provide both formulations here and demonstrate that they are equvialent."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Features"
      ],
      "metadata": {
        "id": "BFAiVa7MhG6N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJQL5cbChKPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmthELBg1A2-"
      },
      "source": [
        "**YAMNet**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_sample_rate(original_sample_rate, waveform,\n",
        "                       desired_sample_rate=16000):\n",
        "  \"\"\"Resample waveform if required. YAMNet needs 16kHz audio and normalized to [-1,1]\"\"\"\n",
        "  if original_sample_rate != desired_sample_rate:\n",
        "    desired_length = int(round(float(len(waveform)) /\n",
        "                               original_sample_rate * desired_sample_rate))\n",
        "    waveform = scipy.signal.resample(waveform, desired_length)\n",
        "\n",
        "    waveform = waveform / tf.int16.max #normalize to [-1,+1]\n",
        "  return desired_sample_rate, waveform"
      ],
      "metadata": {
        "id": "a7eEKWq25WKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load yamnet model\n",
        "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "yamnet_model = hub.load(yamnet_model_handle)"
      ],
      "metadata": {
        "id": "HYlWPKBCbnuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Read csv file as dataframe\n",
        "\n",
        "\"\"\"Change variable 'split_scene' and 'split' according to the data to be used for feature extraction.\n",
        "Repeat cell as needed to get dataset features\"\"\"\n",
        "\n",
        "split_scene = 'test_nature'\n",
        "split = 'test'\n",
        "\n",
        "df = pd.read_csv(f'/content/drive/My Drive/SARdBScene_files/SARdBScene_annotations/{split_scene}.csv')\n",
        "\n",
        "#source_count_labels = df['Source Count'].to_numpy() #already saved source count labels\n",
        "\n",
        "embedding_list=[]\n",
        "files = []\n",
        "for i in range(len(df)):\n",
        "    print(i, '/', len(df))\n",
        "    audiofile = df['Filename'][i].split(\"/\") #get files in order, split string for audio file only not path\n",
        "    f = audiofile[7]\n",
        "    filename = f'/content/drive/My Drive/SARdBScene/{split}/{split_scene}/{f}'\n",
        "\n",
        "    sample_rate1, testing_wav_data = wavfile.read(filename, 'rb')\n",
        "    sample_rate, testing_wav_data = ensure_sample_rate(sample_rate1, testing_wav_data)\n",
        "    score, embedding, spec = yamnet_model(testing_wav_data)\n",
        "\n",
        "    embedding_list.append(embedding)\n",
        "    files.append(filename) #check\n",
        "\n",
        "embedding_list_np = np.array(embedding_list)\n",
        "np.save(f'/content/drive/My Drive/Colab Notebooks/SARdBScene_features/{split_scene}_yamnet.npy',embedding_list_np)\n"
      ],
      "metadata": {
        "id": "DGZigwW9qzJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OpenL3**"
      ],
      "metadata": {
        "id": "qssRzQoLhk5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openl3\n",
        "import soundfile as sf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Gs5MVVmfdy0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Read csv file as dataframe\n",
        "\"\"\"Change variable 'split_scene' and 'split' according to the data to be used for feature extraction.\n",
        "Repeat cell as needed to get dataset features\"\"\"\n",
        "\n",
        "split_scene = 'valid_nature'\n",
        "split = 'valid'\n",
        "\n",
        "df = pd.read_csv(f'/content/drive/My Drive/SARdBScene_files/SARdBScene_annotations/{split_scene}.csv')\n",
        "\n",
        "#source_count_labels = df['Source Count'].to_numpy() #already saved source count labels\n",
        "\n",
        "model = openl3.models.load_audio_embedding_model(input_repr=\"mel256\", content_type=\"env\",embedding_size=512)\n",
        "\n",
        "embedding_list=[]\n",
        "files = []\n",
        "for i in range(len(df)):\n",
        "    print(i+1, '/', len(df))\n",
        "    audiofile = df['Filename'][i].split(\"/\") #get files in order, split string for audio file only not path\n",
        "    filename = audiofile[7]\n",
        "    y,sr = sf.read(f'/content/drive/My Drive/SARdBScene/{split}/{split_scene}/{filename}')#sr=44100\n",
        "    embedding, t2 = openl3.get_audio_embedding(y, sr, model=model,verbose=0)\n",
        "\n",
        "    embedding_list.append(embedding)\n",
        "    files.append(filename) #check\n",
        "\n",
        "embedding_list_np = np.array(embedding_list)\n",
        "np.save(f'/content/drive/My Drive/ColabNotebooks/SARdBScene_features/{split_scene}_openL3.npy',embedding_list_np)\n"
      ],
      "metadata": {
        "id": "oxirr6_2gjSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "Kf-TVwYE4Tmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After hand-crafted features and deep features are saved. Load only the data needed for analysis (shown loads from 4 subsets of sardbscene)"
      ],
      "metadata": {
        "id": "Ac_CxUezFyFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_zcr = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/zcr_train_office.npy\"),\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/zcr_train_urban.npy\"),\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/zcr_train_home.npy\"),\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/zcr_train_nature.npy\")))\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_zcr)\n",
        "train_zcr = scaler.transform(train_zcr)\n",
        "print(train_zcr.shape)\n",
        "\n",
        "#entropy\n",
        "train_ent =np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/spectral_entropy_train_office.npy\"),\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/spectral_entropy_train_urban.npy\"),\n",
        "                           np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/spectral_entropy_train_home.npy\"),\n",
        "                           np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/spectral_entropy_train_nature.npy\")))\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_ent)\n",
        "train_ent = scaler.transform(train_ent)\n",
        "print(train_ent.shape)\n",
        "\n",
        "#cepstral\n",
        "#atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_office.npz\")\n",
        "#mfcc = atr[\"arr_0\"]\n",
        "#mfcc_d = atr[\"arr_1\"]\n",
        "#mfcc_dd = atr[\"arr_2\"]\n",
        "mfcc = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_office.npz\")[\"arr_0\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_urban.npz\")[\"arr_0\"],\n",
        "                           np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_home.npz\")[\"arr_0\"],\n",
        "                           np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_nature.npz\")[\"arr_0\"]))\n",
        "\n",
        "mfcc_d = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_office.npz\")[\"arr_1\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_urban.npz\")[\"arr_1\"],\n",
        "                           np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_home.npz\")[\"arr_1\"],\n",
        "                           np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_nature.npz\")[\"arr_1\"]))\n",
        "\n",
        "mfcc_dd = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_office.npz\")[\"arr_2\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_urban.npz\")[\"arr_2\"],\n",
        "                           np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_home.npz\")[\"arr_2\"],\n",
        "                           np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_nature.npz\")[\"arr_2\"]))\n",
        "\n",
        "mfcc = np.reshape(mfcc,(len(mfcc),mfcc.shape[1]*mfcc.shape[2]))\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(mfcc)\n",
        "mfcc = scaler.transform(mfcc)\n",
        "print(mfcc.shape)\n",
        "\n",
        "mfcc_d = np.reshape(mfcc_d,(len(mfcc_d),mfcc_d.shape[1]*mfcc_d.shape[2]))\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(mfcc_d)\n",
        "mfcc_d = scaler.transform(mfcc_d)\n",
        "print(mfcc_d.shape)\n",
        "\n",
        "mfcc_dd = np.reshape(mfcc_dd,(len(mfcc_dd), mfcc_dd.shape[1]*mfcc_dd.shape[2]))\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(mfcc_dd)\n",
        "mfcc_dd = scaler.transform(mfcc_dd)\n",
        "print(mfcc_dd.shape)"
      ],
      "metadata": {
        "id": "9gjjfdTnAY7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cka(gram_linear(mfcc), gram_linear(mfcc_dd))"
      ],
      "metadata": {
        "id": "xZyuY4YrJhQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cka(gram_linear(mfcc), gram_linear(mfcc))"
      ],
      "metadata": {
        "id": "qym9Zj2XKUP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_nature.npz\")\n",
        "# npz file saved with rms, cs, bw, fl, roll\n",
        "# acoustic features are 1x431 dimensional\n",
        "train_rms = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_office.npz\")[\"arr_0\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_urban.npz\")[\"arr_0\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_home.npz\")[\"arr_0\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_nature.npz\")[\"arr_0\"]))\n",
        "\n",
        "train_cs = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_office.npz\")[\"arr_1\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_urban.npz\")[\"arr_1\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_home.npz\")[\"arr_1\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_nature.npz\")[\"arr_1\"]))\n",
        "\n",
        "train_bw = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_office.npz\")[\"arr_2\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_urban.npz\")[\"arr_2\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_home.npz\")[\"arr_2\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_nature.npz\")[\"arr_2\"]))\n",
        "\n",
        "train_fl = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_office.npz\")[\"arr_3\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_urban.npz\")[\"arr_3\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_home.npz\")[\"arr_3\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_nature.npz\")[\"arr_3\"]))\n",
        "\n",
        "train_roll = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_office.npz\")[\"arr_4\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_urban.npz\")[\"arr_4\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_home.npz\")[\"arr_4\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustics_train_nature.npz\")[\"arr_4\"]))\n",
        "\n",
        "\n",
        "## Normalize all features\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_rms)\n",
        "train_rms = scaler.transform(train_rms)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_cs)\n",
        "train_cs = scaler.transform(train_cs)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_bw)\n",
        "train_bw = scaler.transform(train_bw)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_fl)\n",
        "train_fl = scaler.transform(train_fl)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_roll)\n",
        "train_roll = scaler.transform(train_roll)\n",
        "\n",
        "power_tr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_nature.npz\")\n",
        "# spectrogram features of median power vs time (medp_time), meanp_time, tp80, tp75, tp70\n",
        "# power features are 1x431 dimensional, time to dB features are 1x64\n",
        "train_medp = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_office.npz\")[\"arr_0\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_urban.npz\")[\"arr_0\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_home.npz\")[\"arr_0\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_nature.npz\")[\"arr_0\"]))\n",
        "\n",
        "train_meanp = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_office.npz\")[\"arr_1\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_urban.npz\")[\"arr_1\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_home.npz\")[\"arr_1\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_nature.npz\")[\"arr_1\"]))\n",
        "\n",
        "train_tp80 = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_office.npz\")[\"arr_2\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_urban.npz\")[\"arr_2\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_home.npz\")[\"arr_2\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_nature.npz\")[\"arr_2\"]))\n",
        "\n",
        "train_tp75 = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_office.npz\")[\"arr_3\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_urban.npz\")[\"arr_3\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_home.npz\")[\"arr_3\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_nature.npz\")[\"arr_3\"]))\n",
        "\n",
        "train_tp70 = np.concatenate((np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_office.npz\")[\"arr_4\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_urban.npz\")[\"arr_4\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_home.npz\")[\"arr_4\"],\n",
        "                            np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/power_time_train_nature.npz\")[\"arr_4\"]))\n",
        "\n",
        "## Normalize all features\n",
        "\n",
        "scaler =  StandardScaler(with_mean=True, with_std=True).fit(train_medp)\n",
        "train_medp = scaler.transform(train_medp)\n",
        "\n",
        "scaler =  StandardScaler(with_mean=True, with_std=True).fit(train_meanp)\n",
        "train_meanp = scaler.transform(train_meanp)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_tp80)\n",
        "train_tp80 = scaler.transform(train_tp80)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_tp75)\n",
        "train_tp75 = scaler.transform(train_tp75)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_tp70)\n",
        "train_tp70 = scaler.transform(train_tp70)\n",
        "\n",
        "#\n",
        "# wavelets\n",
        "wave_nature = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_nature.npy')\n",
        "wave_office = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_office.npy')\n",
        "wave_urban = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_urban.npy')\n",
        "wave_home = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_home.npy')\n",
        "\n",
        "wave1_tr = np.concatenate((wave_office, wave_urban, wave_home, wave_nature))\n",
        "# wavelet width 1, features per row as mean, meadian, std, var, kurtosis, 25th quantile, 75th quantile\n",
        "# wavelet features are all 1x64 dimensional vectors\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[0,:,:])\n",
        "train_wave1_mean = scaler.transform(wave1_tr[0,:,:])\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[1,:,:])\n",
        "train_wave1_med = scaler.transform(wave1_tr[1,:,:])\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[2,:,:])\n",
        "train_wave1_std = scaler.transform(wave1_tr[2,:,:])\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[3,:,:])\n",
        "train_wave1_var = scaler.transform(wave1_tr[3,:,:])\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[4,:,:])\n",
        "train_wave1_kur = scaler.transform(wave1_tr[4,:,:])\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[5,:,:])\n",
        "train_wave1_q25 = scaler.transform(wave1_tr[5,:,:])\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[6,:,:])\n",
        "train_wave1_q75 = scaler.transform(wave1_tr[6,:,:])"
      ],
      "metadata": {
        "id": "Z-GZwXm0LSyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CKA Similarity"
      ],
      "metadata": {
        "id": "NvGtMZZ84WBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteratively calculate linear CKA metric for each combination of hand-crafted feature and deep feature set. Shown for using the `Nature' subset of SARdBScene."
      ],
      "metadata": {
        "id": "NWfNa6EQGVCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Add random noise for baseline\n",
        "train_random = np.random.normal(0, 1,\n",
        "                                size = train_rms.shape[0]*train_rms.shape[1]).reshape(train_rms.shape)\n",
        "\n",
        "# list of features for easier access of cka operation\n",
        "spec_features = [train_zcr, train_ent, train_rms, train_cs, train_bw, train_fl, train_roll,\n",
        "                 train_medp, train_meanp, train_tp80, train_tp75, train_tp70,\n",
        "                 mfcc, mfcc_d, mfcc_dd,\n",
        "                 train_wave1_mean, train_wave1_med, train_wave1_std, train_wave1_var,\n",
        "                 train_wave1_kur, train_wave1_q25, train_wave1_q75, train_random]\n",
        "\n",
        "ckatrain = np.zeros((2, len(spec_features), 1))\n",
        "\n",
        "# CKA for pretrain audio features and hand crafted acoustics (spec_features variable)\n",
        "\n",
        "pp = '/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_'\n",
        "arch_list = ['yamnet', 'openL3']\n",
        "for k in range(1): ## inits\n",
        "    for i in range(len(arch_list)):\n",
        "        ## i - architecture\n",
        "        feat = np.load(pp+arch_list[i]+'.npy')\n",
        "        feat = np.mean(feat,axis=1)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True).fit(feat)\n",
        "        feat = scaler.transform(feat)\n",
        "        ## Flatten and scale\n",
        "\n",
        "        for j in range(len(spec_features)):\n",
        "            ## j - hand-crafted feature\n",
        "            ## Calculate linear CKA\n",
        "            #ckatrain[i,j,k] = linear_CKA2(feat, spec_features[j])\n",
        "            ckatrain[i,j,k] = cka(gram_linear(feat), gram_linear(spec_features[j]))\n",
        "\n",
        "            print(i, j, k)\n",
        "\n",
        "yticklabels = ['YAMNet', 'OpenL3']\n",
        "xticklabels = ['ZCR', 'Spec. Entropy', 'RMS', 'Spec. Centroid', 'Spec. Bandwidth', 'Spec. Flatness', 'Spec. Rolloff',\n",
        "               'Median Power', 'Mean Power', 'Time to -80dB',\n",
        "               'Time to -75dB','Time to -70dB',\n",
        "               'MFCC', 'MFCC-delta', 'MFCC-delta delta',\n",
        "               'Mean Wavelet-1',\n",
        "               'Median Wavelet-1', 'Std. Wavelet-1','Var. Wavelet-1','Kurtosis Wavelet-1',\n",
        "               '25th Quantile Wavelet-1', '75th Quantile Wavelet-1','Random']\n",
        "plt.figure(figsize = (30,10))\n",
        "sns.heatmap(np.round(np.mean(ckatrain, axis = 2), 2),  vmin = 0, vmax = 1, annot=True,\n",
        "           xticklabels = xticklabels, yticklabels = yticklabels)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "bEOmMrgFP_fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#differenet sized plot, no values displayed in cells\n",
        "plt.figure(figsize = (30,5))\n",
        "sns.heatmap(np.round(np.mean(ckatrain, axis = 2), 2),  vmin = 0, vmax = 1, annot=False,\n",
        "           xticklabels = xticklabels, yticklabels = yticklabels)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "GgylrptYY-b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### VERTICAL PLOT\n",
        "models = ['YAMNet', 'OpenL3']\n",
        "feats = ['ZCR', 'Spec. Entropy', 'RMS', 'Spec. Centroid', 'Spec. Bandwidth', 'Spec. Flatness', 'Spec. Rolloff',\n",
        "               'Median Power', 'Mean Power', 'Time to -80dB',\n",
        "               'Time to -75dB','Time to -70dB',\n",
        "               'MFCC', 'MFCC-delta', 'MFCC-delta delta',\n",
        "               'Mean Wavelet-1',\n",
        "               'Median Wavelet-1', 'Std. Wavelet-1','Var. Wavelet-1','Kurtosis Wavelet-1',\n",
        "               '25th Quantile Wavelet-1', '75th Quantile Wavelet-1','Random']\n",
        "\n",
        "plt.figure(figsize = (20,30))\n",
        "sns.heatmap(ckatrain.T,  vmin = 0, vmax = 1, annot=False,\n",
        "           xticklabels = models, yticklabels = feats)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nz2E-PDPHN8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MFCC statistics"
      ],
      "metadata": {
        "id": "UtZOQPX1G6_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kurtosis\n",
        "\n",
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/cepstral_train_nature.npz\")\n",
        "mfcc = atr[\"arr_0\"]\n",
        "mfcc_d = atr[\"arr_1\"]\n",
        "mfcc_dd = atr[\"arr_2\"]\n",
        "r = mfcc.shape[1]\n",
        "c = mfcc.shape[2]\n",
        "\n",
        "mfcc = np.reshape(mfcc,(len(mfcc),r*c))\n",
        "mfcc_d = np.reshape(mfcc_d,(len(mfcc),r*c))\n",
        "mfcc_dd = np.reshape(mfcc_dd,(len(mfcc),r*c))\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(mfcc)\n",
        "mfcc = scaler.transform(mfcc)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(mfcc_d)\n",
        "mfcc_d = scaler.transform(mfcc_d)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(mfcc_dd)\n",
        "mfcc_dd = scaler.transform(mfcc_dd)\n",
        "\n",
        "mfcc = np.reshape(mfcc,(len(mfcc),r,c))\n",
        "mfcc_d = np.reshape(mfcc_d,(len(mfcc),r,c))\n",
        "mfcc_dd = np.reshape(mfcc_dd,(len(mfcc),r,c))\n",
        "\n",
        "mfcc_meanf = np.mean(mfcc,axis=1) #mel band mean, so avg of the freq. for each time frame\n",
        "mfcc_stdf = np.std(mfcc,axis=1)\n",
        "mfcc_medf = np.median(mfcc,axis=1)\n",
        "mfcc_varf = np.var(mfcc,axis=1)\n",
        "mfcc_kurf = kurtosis(mfcc,axis=1)\n",
        "\n",
        "mfcc_meant = np.mean(mfcc,axis=2)\n",
        "mfcc_stdt = np.std(mfcc,axis=2)\n",
        "mfcc_medt = np.median(mfcc,axis=2)\n",
        "mfcc_vart = np.var(mfcc,axis=2)\n",
        "mfcc_kurt = kurtosis(mfcc,axis=2)\n",
        "\n",
        "#delta\n",
        "mfcc_d_meanf = np.mean(mfcc_d,axis=1) #mel band mean, so avg of the freq. for each time frame\n",
        "mfcc_d_stdf = np.std(mfcc_d,axis=1)\n",
        "mfcc_d_medf = np.median(mfcc_d,axis=1)\n",
        "mfcc_d_varf = np.var(mfcc_d,axis=1)\n",
        "mfcc_d_kurf = kurtosis(mfcc_d,axis=1)\n",
        "\n",
        "mfcc_d_meant = np.mean(mfcc_d,axis=2)\n",
        "mfcc_d_stdt = np.std(mfcc_d,axis=2)\n",
        "mfcc_d_medt = np.median(mfcc_d,axis=2)\n",
        "mfcc_d_vart = np.var(mfcc_d,axis=2)\n",
        "mfcc_d_kurt = kurtosis(mfcc_d,axis=2)\n",
        "\n",
        "#double delta\n",
        "mfcc_dd_meanf = np.mean(mfcc_dd,axis=1) #mel band mean, so avg of the freq. for each time frame\n",
        "mfcc_dd_stdf = np.std(mfcc_dd,axis=1)\n",
        "mfcc_dd_medf = np.median(mfcc_dd,axis=1)\n",
        "mfcc_dd_varf = np.var(mfcc_dd,axis=1)\n",
        "mfcc_dd_kurf = kurtosis(mfcc_dd,axis=1)\n",
        "\n",
        "mfcc_dd_meant = np.mean(mfcc_dd,axis=2)\n",
        "mfcc_dd_stdt = np.std(mfcc_dd,axis=2)\n",
        "mfcc_dd_medt = np.median(mfcc_dd,axis=2)\n",
        "mfcc_dd_vart = np.var(mfcc_dd,axis=2)\n",
        "mfcc_dd_kurt = kurtosis(mfcc_dd,axis=2)"
      ],
      "metadata": {
        "id": "FvEOQs3LaYQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mfcc_meanf.shape)\n",
        "print(mfcc_meant.shape)"
      ],
      "metadata": {
        "id": "W1vp1gwiq0CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meanf = np.concatenate((mfcc_meanf,mfcc_d_meanf,mfcc_dd_meanf),axis=1)\n",
        "meant = np.concatenate((mfcc_meant,mfcc_d_meant,mfcc_dd_meant),axis=1)\n",
        "\n",
        "stdf = np.concatenate((mfcc_stdf,mfcc_d_stdf,mfcc_dd_stdf),axis=1)\n",
        "stdt = np.concatenate((mfcc_stdt,mfcc_d_stdt,mfcc_dd_stdt),axis=1)\n",
        "\n",
        "medf = np.concatenate((mfcc_medf,mfcc_d_medf,mfcc_dd_medf),axis=1)\n",
        "medt = np.concatenate((mfcc_medt,mfcc_d_medt,mfcc_dd_medt),axis=1)\n",
        "\n",
        "varf = np.concatenate((mfcc_varf,mfcc_d_varf,mfcc_dd_varf),axis=1)\n",
        "vart = np.concatenate((mfcc_vart,mfcc_d_vart,mfcc_dd_vart),axis=1)\n",
        "\n",
        "kurf = np.concatenate((mfcc_kurf,mfcc_d_kurf,mfcc_dd_kurf),axis=1)\n",
        "kurt = np.concatenate((mfcc_kurt,mfcc_d_kurt,mfcc_dd_kurt),axis=1)"
      ],
      "metadata": {
        "id": "pJheHjW-9upj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Add random noise for baseline\n",
        "#train_random = np.random.normal(0, 1,size = mfcc.shape[0]*train_wave1_mean.shape[1]).reshape(train_wave1_mean.shape)\n",
        "\n",
        "# list of features for easier access of cka operation\n",
        "spec_features = [mfcc_meanf, mfcc_stdf, mfcc_medf, mfcc_varf ,mfcc_kurf,\n",
        "              mfcc_meant ,mfcc_stdt , mfcc_medt , mfcc_vart, mfcc_kurt,\n",
        "              mfcc_d_meanf, mfcc_d_stdf, mfcc_d_medf, mfcc_d_varf, mfcc_d_kurf,\n",
        "              mfcc_d_meant, mfcc_d_stdt, mfcc_d_medt, mfcc_d_vart,mfcc_d_kurt,\n",
        "              mfcc_dd_meanf, mfcc_dd_stdf,mfcc_dd_medf , mfcc_dd_varf ,mfcc_dd_kurf,\n",
        "              mfcc_dd_meant, mfcc_dd_stdt, mfcc_dd_medt ,mfcc_dd_vart,mfcc_dd_kurt]\n",
        "\n",
        "\n",
        "ckatrain = np.zeros((2, len(spec_features), 1)) #7 feats per wavelet width x 4 widths = 28 +1 for random\n",
        "\n",
        "# CKA for pretrain audio features and hand crafted acoustics (spec_features variable)\n",
        "\n",
        "pp = '/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_'\n",
        "arch_list = ['yamnet', 'openL3']\n",
        "for k in range(1): ## inits\n",
        "    for i in range(len(arch_list)):\n",
        "        ## i - architecture\n",
        "        feat = np.load(pp+arch_list[i]+'.npy')\n",
        "        feat = np.reshape(feat, (len(feat), feat.shape[1]*feat.shape[2]))\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True).fit(feat)\n",
        "        feat = scaler.transform(feat)\n",
        "        ## Flatten and scale\n",
        "\n",
        "        for j in range(len(spec_features)):\n",
        "            ## j - hand-crafted feature\n",
        "            ## Calculate linear CKA\n",
        "            #ckatrain[i,j,k] = linear_CKA2(feat, spec_features[j])\n",
        "            ckatrain[i,j,k] = cka(gram_linear(feat), gram_linear(spec_features[j]))\n",
        "\n",
        "            print(i, j, k)\n",
        "\n",
        "#plot\n",
        "yticklabels = ['YAMNet', 'OpenL3']\n",
        "xticklabels = ['mfcc_meanf', 'mfcc_stdf', 'mfcc_medf', 'mfcc_varf' , 'mfcc_kurf',\n",
        "              'mfcc_meant' ,'mfcc_stdt' , 'mfcc_medt' , 'mfcc_vart', 'mfcc_kurt',\n",
        "              'mfcc_d_meanf', 'mfcc_d_stdf', 'mfcc_d_medf', 'mfcc_d_varf', 'mfcc_d_kurf',\n",
        "              'mfcc_d_meant', 'mfcc_d_stdt', 'mfcc_d_medt', 'mfcc_d_vart','mfcc_d_kurt',\n",
        "              'mfcc_dd_meanf', 'mfcc_dd_stdf','mfcc_dd_medf' , 'mfcc_dd_varf' ,'mfcc_dd_kurf',\n",
        "              'mfcc_dd_meant', 'mfcc_dd_stdt', 'mfcc_dd_medt' ,'mfcc_dd_vart','mfcc_dd_kurt']\n",
        "\n",
        "plt.figure(figsize = (30,10))\n",
        "sns.heatmap(np.round(np.mean(ckatrain, axis = 2), 2),  vmin = 0, vmax = 1, annot=True,\n",
        "           xticklabels = xticklabels, yticklabels = yticklabels)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "aQ-lqD7Rsh0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification\n",
        "perform classification of HCF with MLP classifiers"
      ],
      "metadata": {
        "id": "wYmYYhiU4Yoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 75th quantile wavelet"
      ],
      "metadata": {
        "id": "ADFU2SFMLSCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load and organize data\n",
        "wave1_tr = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_nature.npy')\n",
        "wave1_v = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_valid_nature.npy')\n",
        "wave1_te = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_test_nature.npy')\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[6,:,:])\n",
        "train_wave1_q75 = scaler.transform(wave1_tr[6,:,:])\n",
        "valid_wave1_q75 = scaler.transform(wave1_v[6,:,:])\n",
        "test_wave1_q75 = scaler.transform(wave1_te[6,:,:])\n",
        "\n",
        "X_train = train_wave1_q75\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "X_valid =  valid_wave1_q75\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "X_test = test_wave1_q75\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "for i in range(5): #experiment 5 times\n",
        "  print(\"Round \",i+1)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  #y = GlobalAveragePooling1D(data_format='channels_last')(input_this)\n",
        "  #y = Dense(512,activation='relu')(y)\n",
        "  y = Dense(256,activation='relu')(input_this)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "  preds = model.predict(X_test)\n",
        "\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "_iYN27QACie9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2layer MLP"
      ],
      "metadata": {
        "id": "nyEVkJpCJOGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wave1_tr = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_nature.npy')\n",
        "wave1_v = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_valid_nature.npy')\n",
        "wave1_te = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_test_nature.npy')\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[6,:,:])\n",
        "train_wave1_q75 = scaler.transform(wave1_tr[6,:,:])\n",
        "valid_wave1_q75 = scaler.transform(wave1_v[6,:,:])\n",
        "test_wave1_q75 = scaler.transform(wave1_te[6,:,:])\n",
        "\n",
        "X_train = train_wave1_q75\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "X_valid =  valid_wave1_q75\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "X_test = test_wave1_q75\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "for i in range(5):\n",
        "  print(\"Round \",i+1)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  #y = GlobalAveragePooling1D(data_format='channels_last')(input_this)\n",
        "  y = Dense(256,activation='relu')(input_this)\n",
        "  y = Dense(128,activation='relu')(y)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "  preds = model.predict(X_test)\n",
        "\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "FfGHwxEfIwSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3layer MLP"
      ],
      "metadata": {
        "id": "totAULK6JLqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wave1_tr = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_nature.npy')\n",
        "wave1_v = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_valid_nature.npy')\n",
        "wave1_te = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_test_nature.npy')\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[6,:,:])\n",
        "train_wave1_q75 = scaler.transform(wave1_tr[6,:,:])\n",
        "valid_wave1_q75 = scaler.transform(wave1_v[6,:,:])\n",
        "test_wave1_q75 = scaler.transform(wave1_te[6,:,:])\n",
        "\n",
        "X_train = train_wave1_q75\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "X_valid =  valid_wave1_q75\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "X_test = test_wave1_q75\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "for i in range(5):\n",
        "  print(\"Round \",i+1)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  #y = GlobalAveragePooling1D(data_format='channels_last')(input_this)\n",
        "  y = Dense(256,activation='relu')(input_this)\n",
        "  y = Dense(128,activation='relu')(y)\n",
        "  y = Dense(64,activation='relu')(y)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "  preds = model.predict(X_test)\n",
        "\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "krZgHQ3SI9Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "very small MLP"
      ],
      "metadata": {
        "id": "kzcZr9L3JJS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wave1_tr = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_nature.npy')\n",
        "wave1_v = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_valid_nature.npy')\n",
        "wave1_te = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_test_nature.npy')\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[6,:,:])\n",
        "train_wave1_q75 = scaler.transform(wave1_tr[6,:,:])\n",
        "valid_wave1_q75 = scaler.transform(wave1_v[6,:,:])\n",
        "test_wave1_q75 = scaler.transform(wave1_te[6,:,:])\n",
        "\n",
        "X_train = train_wave1_q75\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "X_valid =  valid_wave1_q75\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "X_test = test_wave1_q75\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "for i in range(5):\n",
        "  print(\"Round \",i+1)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  #y = GlobalAveragePooling1D(data_format='channels_last')(input_this)\n",
        "  y = Dense(32,activation='relu')(input_this)\n",
        "\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "  preds = model.predict(X_test)\n",
        "\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "pnBFR9kPJEio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatness *"
      ],
      "metadata": {
        "id": "QZAPvvH2C_VM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_C_8b6jCie-"
      },
      "outputs": [],
      "source": [
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_nature.npz\")\n",
        "# npz file saved with rms, cs, bw, fl, roll\n",
        "# acoustic features are 1x431 dimensional\n",
        "X_train = atr['arr_3']\n",
        "\n",
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_valid_nature.npz\")\n",
        "X_valid = atr['arr_3']\n",
        "\n",
        "\n",
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_test_nature.npz\")\n",
        "X_test = atr['arr_3']\n",
        "\n",
        "\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "\n",
        "for i in range(5):\n",
        "  print('Round ',i)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  y = Dense(256,activation='relu')(input_this)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "  model.summary()\n",
        "\n",
        "  # training\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "\n",
        "  #  evaluating\n",
        "  preds = model.predict(X_test)\n",
        "\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "  del model\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "print(acc_perclass)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2layer MLP\n",
        "avg and std: 32.32$\\pm$2.57\\\\"
      ],
      "metadata": {
        "id": "52xGAAeqN-r_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0A6jppdN8l5"
      },
      "outputs": [],
      "source": [
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_nature.npz\")\n",
        "# npz file saved with rms, cs, bw, fl, roll\n",
        "# acoustic features are 1x431 dimensional\n",
        "X_train = atr['arr_3']\n",
        "\n",
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_valid_nature.npz\")\n",
        "X_valid = atr['arr_3']\n",
        "\n",
        "\n",
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_test_nature.npz\")\n",
        "X_test = atr['arr_3']\n",
        "\n",
        "\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "\n",
        "for i in range(5):\n",
        "  print('Round ',i)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  y = Dense(256,activation='relu')(input_this)\n",
        "  y = Dense(128,activation='relu')(y)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "  model.summary()\n",
        "\n",
        "  # training\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "\n",
        "  #  evaluating\n",
        "  preds = model.predict(X_test)\n",
        "\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "  del model\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3layer MLP\n",
        "\n",
        "34.74$\\pm$2.72"
      ],
      "metadata": {
        "id": "o69xC27nOTlc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov5PNxNJOJcI"
      },
      "outputs": [],
      "source": [
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_nature.npz\")\n",
        "# npz file saved with rms, cs, bw, fl, roll\n",
        "# acoustic features are 1x431 dimensional\n",
        "X_train = atr['arr_3']\n",
        "\n",
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_valid_nature.npz\")\n",
        "X_valid = atr['arr_3']\n",
        "\n",
        "\n",
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_test_nature.npz\")\n",
        "X_test = atr['arr_3']\n",
        "\n",
        "\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "\n",
        "for i in range(5):\n",
        "  print('Round ',i)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  y = Dense(256,activation='relu')(input_this)\n",
        "  y = Dense(128,activation='relu')(y)\n",
        "  y = Dense(64,activation='relu')(y)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "  model.summary()\n",
        "\n",
        "  # training\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "\n",
        "  #  evaluating\n",
        "  preds = model.predict(X_test)\n",
        "\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "  del model\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "very small MLP"
      ],
      "metadata": {
        "id": "xhJkvkUnHOer"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-nnPsJxHcQh"
      },
      "outputs": [],
      "source": [
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_train_nature.npz\")\n",
        "# npz file saved with rms, cs, bw, fl, roll\n",
        "# acoustic features are 1x431 dimensional\n",
        "X_train = atr['arr_3']\n",
        "\n",
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_valid_nature.npz\")\n",
        "X_valid = atr['arr_3']\n",
        "\n",
        "\n",
        "atr = np.load(\"/content/drive/My Drive/Colab Notebooks/sardbscene_xai/acoustic_test_nature.npz\")\n",
        "X_test = atr['arr_3']\n",
        "\n",
        "\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "for i in range(5):\n",
        "  print('Round ',i)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  y = Dense(32,activation='relu')(input_this)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "  model.summary()\n",
        "\n",
        "  # training\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "\n",
        "  #  evaluating\n",
        "  preds = model.predict(X_test)\n",
        "\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "  del model\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "print(acc_perclass)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 25th quantile"
      ],
      "metadata": {
        "id": "C_GJZaOtDl2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wave1_tr = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_nature.npy')\n",
        "wave1_v = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_valid_nature.npy')\n",
        "wave1_te = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_test_nature.npy')\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[5,:,:])\n",
        "train_wave1_q75 = scaler.transform(wave1_tr[5,:,:])\n",
        "valid_wave1_q75 = scaler.transform(wave1_v[5,:,:])\n",
        "test_wave1_q75 = scaler.transform(wave1_te[5,:,:])\n",
        "\n",
        "X_train = train_wave1_q75\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "X_valid =  valid_wave1_q75\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "X_test = test_wave1_q75\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "for i in range(5):\n",
        "  print(\"Round \",i+1)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  #y = GlobalAveragePooling1D(data_format='channels_last')(input_this)\n",
        "  #y = Dense(512,activation='relu')(y)\n",
        "  y = Dense(256,activation='relu')(input_this)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "  model.summary()\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "  preds = model.predict(X_test)\n",
        "  del model\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "dEbNzfs0Lmjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2layer MLP"
      ],
      "metadata": {
        "id": "vWHSoIa8Lmjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wave1_tr = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_nature.npy')\n",
        "wave1_v = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_valid_nature.npy')\n",
        "wave1_te = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_test_nature.npy')\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[5,:,:])\n",
        "train_wave1_q75 = scaler.transform(wave1_tr[5,:,:])\n",
        "valid_wave1_q75 = scaler.transform(wave1_v[5,:,:])\n",
        "test_wave1_q75 = scaler.transform(wave1_te[5,:,:])\n",
        "\n",
        "X_train = train_wave1_q75\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "X_valid =  valid_wave1_q75\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "X_test = test_wave1_q75\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "for i in range(5):\n",
        "  print(\"Round \",i+1)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  #y = GlobalAveragePooling1D(data_format='channels_last')(input_this)\n",
        "  y = Dense(256,activation='relu')(input_this)\n",
        "  y = Dense(128,activation='relu')(y)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "  model.summary()\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "  preds = model.predict(X_test)\n",
        "  del model\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "17YZ9XFdLmjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3layer MLP"
      ],
      "metadata": {
        "id": "8skiRnpTLmjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wave1_tr = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_nature.npy')\n",
        "wave1_v = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_valid_nature.npy')\n",
        "wave1_te = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_test_nature.npy')\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[5,:,:])\n",
        "train_wave1_q75 = scaler.transform(wave1_tr[5,:,:])\n",
        "valid_wave1_q75 = scaler.transform(wave1_v[5,:,:])\n",
        "test_wave1_q75 = scaler.transform(wave1_te[5,:,:])\n",
        "\n",
        "X_train = train_wave1_q75\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "X_valid =  valid_wave1_q75\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "X_test = test_wave1_q75\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "for i in range(5):\n",
        "  print(\"Round \",i+1)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  #y = GlobalAveragePooling1D(data_format='channels_last')(input_this)\n",
        "  y = Dense(256,activation='relu')(input_this)\n",
        "  y = Dense(128,activation='relu')(y)\n",
        "  y = Dense(64,activation='relu')(y)\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "  model.summary()\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "  preds = model.predict(X_test)\n",
        "  del model\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "IRL71A0sLmjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "very small MLP"
      ],
      "metadata": {
        "id": "AkBmubVnLmjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wave1_tr = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_train_nature.npy')\n",
        "wave1_v = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_valid_nature.npy')\n",
        "wave1_te = np.load('/content/drive/My Drive/Colab Notebooks/sardbscene_xai/addwavelets1_test_nature.npy')\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True).fit(wave1_tr[5,:,:])\n",
        "train_wave1_q75 = scaler.transform(wave1_tr[5,:,:])\n",
        "valid_wave1_q75 = scaler.transform(wave1_v[5,:,:])\n",
        "test_wave1_q75 = scaler.transform(wave1_te[5,:,:])\n",
        "\n",
        "X_train = train_wave1_q75\n",
        "Y_train = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/train_nature_sc_labels.npy')\n",
        "train_sc_labelcat = keras.utils.to_categorical(Y_train-1)\n",
        "\n",
        "X_valid =  valid_wave1_q75\n",
        "Y_valid = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/valid_nature_sc_labels.npy')\n",
        "valid_sc_labelcat = keras.utils.to_categorical(Y_valid-1)\n",
        "\n",
        "X_test = test_wave1_q75\n",
        "Y_test = np.load('/content/drive/My Drive/Colab Notebooks/SARdBScene_features/test_nature_sc_labels.npy')\n",
        "test_sc_labelcat = keras.utils.to_categorical(Y_test-1)\n",
        "\n",
        "numClasses=12\n",
        "class_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
        "\n",
        "acc = []\n",
        "acc_perclass = []\n",
        "for i in range(5):\n",
        "  print(\"Round \",i+1)\n",
        "  input_this = Input(shape=(X_train[0].shape))\n",
        "  print(input_this)\n",
        "\n",
        "  #y = GlobalAveragePooling1D(data_format='channels_last')(input_this)\n",
        "  y = Dense(32,activation='relu')(input_this)\n",
        "\n",
        "  out = Dense(numClasses, activation='softmax')(y) #for source counting\n",
        "\n",
        "  model = Model(input_this,out)\n",
        "\n",
        "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics='acc')\n",
        "  model.summary()\n",
        "  es = EarlyStopping(monitor='val_loss',verbose=1,patience=10,restore_best_weights=True)\n",
        "  history = model.fit(X_train, train_sc_labelcat, validation_data=(X_valid,valid_sc_labelcat),\n",
        "                    batch_size=64, epochs=100, callbacks=[es])\n",
        "\n",
        "  model_plot(history)\n",
        "  preds = model.predict(X_test)\n",
        "  del model\n",
        "  results(preds, test_sc_labelcat,class_names)\n",
        "\n",
        "  a, b = pa1(preds,test_sc_labelcat,numClasses)\n",
        "\n",
        "  acc.append(a)\n",
        "  acc_perclass.append(b)\n",
        "\n",
        "print(\"PA1 avg and std \", np.mean(acc),\"\", np.std(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "--SBELVcLmjn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}